import asyncio
import json
from playwright.async_api import async_playwright
from src.processor import process_listing

async def scrape_chrono24(max_listings=5):
    """Scrapea anuncios de Submariner en Chrono24"""
    
    async with async_playwright() as p:
        print("üåê Abriendo navegador...")
        browser = await p.chromium.launch(
            headless=False,
            slow_mo=1000  # M√°s lento, m√°s humano
        )
        
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        page = await context.new_page()
        
        # Timeout m√°s largo
        page.set_default_timeout(60000)
        
        url = "https://www.chrono24.es/rolex/submariner-date--mod981.htm"
        print(f"üìç Navegando a Chrono24 (puede tardar 30-60s)...")
        
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            await asyncio.sleep(5)
            print("‚úì P√°gina cargada")
        except Exception as e:
            print(f"‚ùå Error cargando p√°gina: {e}")
            await browser.close()
            return []
        
        listings = []
        
        print(f"üîç Buscando anuncios...\n")
        
        # Scroll para cargar contenido
        await page.evaluate("window.scrollTo(0, 1000)")
        await asyncio.sleep(2)
        
        # Extrae links
        links = await page.query_selector_all('a[href*="submariner"]')
        print(f"Encontrados {len(links)} links")
        
        urls_visited = set()
        
        for i, link in enumerate(links[:max_listings * 3], 1):  # Intenta m√°s por si fallan
            try:
                href = await link.get_attribute('href')
                if not href or 'id' not in href or href in urls_visited:
                    continue
                
                urls_visited.add(href)
                
                full_url = f"https://www.chrono24.es{href}" if href.startswith('/') else href
                
                print(f"[{len(listings)+1}/{max_listings}] Extrayendo: {href[:50]}...")
                
                await page.goto(full_url, wait_until="domcontentloaded", timeout=30000)
                await asyncio.sleep(2)
                
                # Extrae texto
                content = await page.content()
                
                # Texto simplificado
                title = await page.query_selector('h1')
                title_text = await title.inner_text() if title else ""
                
                body = await page.query_selector('body')
                body_text = await body.inner_text() if body else ""
                
                listing_text = f"{title_text}\n{body_text[:2000]}"
                
                print(f"‚úì Extra√≠do ({len(listing_text)} caracteres)")
                listings.append(listing_text)
                
                if len(listings) >= max_listings:
                    break
                
                # Vuelve atr√°s
                await page.go_back(wait_until="domcontentloaded")
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Error en anuncio: {str(e)[:50]}")
                continue
        
        await browser.close()
        return listings

async def main():
    raw_listings = await scrape_chrono24(max_listings=3)  # Solo 3 para probar
    
    if not raw_listings:
        print("‚ùå No se pudieron extraer anuncios")
        return
    
    print(f"\n‚úì {len(raw_listings)} anuncios extra√≠dos")
    print("\nü§ñ Procesando con IA...\n")
    
    results = []
    for i, listing in enumerate(raw_listings, 1):
        print(f"[{i}/{len(raw_listings)}] Procesando con IA...")
        result = process_listing(listing)
        if result:
            results.append(result)
            ref = result['identidad']['referencia']
            precio = result['precio']['precio_anuncio']
            print(f"‚úì {ref} - {precio}‚Ç¨\n")
    
    with open('data/scraped_watches.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ {len(results)} relojes guardados en data/scraped_watches.json")

if __name__ == "__main__":
    asyncio.run(main())
